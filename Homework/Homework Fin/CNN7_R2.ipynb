{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721b64bc-ae4a-4174-a092-6387ec92657b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375ab659-a346-4fca-a07e-9d02cf959111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2ad23e-b3c6-45a9-bea1-3b0a90148722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/300], Loss: 1.4632187949879396\n",
      "Epoch [2/300], Loss: 1.1980260968055871\n",
      "Epoch [3/300], Loss: 1.0893741055675175\n",
      "Epoch [4/300], Loss: 1.0323071856327983\n",
      "Epoch [5/300], Loss: 0.9888744435042066\n",
      "Epoch [6/300], Loss: 0.9557599480194814\n",
      "Epoch [7/300], Loss: 0.9328523074726924\n",
      "Epoch [8/300], Loss: 0.9057292453849407\n",
      "Epoch [9/300], Loss: 0.8929975067867952\n",
      "Epoch [10/300], Loss: 0.8699284452382866\n",
      "Epoch [11/300], Loss: 0.8552517491914428\n",
      "Epoch [12/300], Loss: 0.844140020554023\n",
      "Epoch [13/300], Loss: 0.8275549498284259\n",
      "Epoch [14/300], Loss: 0.8177754930637376\n",
      "Epoch [15/300], Loss: 0.8066412126240523\n",
      "Epoch [16/300], Loss: 0.7945711970939051\n",
      "Epoch [17/300], Loss: 0.7900855072090388\n",
      "Epoch [18/300], Loss: 0.7770882659327344\n",
      "Epoch [19/300], Loss: 0.7747808933029394\n",
      "Epoch [20/300], Loss: 0.7582680306318775\n",
      "Epoch [21/300], Loss: 0.7611302339741032\n",
      "Epoch [22/300], Loss: 0.7455419183463392\n",
      "Epoch [23/300], Loss: 0.7394099558710747\n",
      "Epoch [24/300], Loss: 0.7348670446125748\n",
      "Epoch [25/300], Loss: 0.7290238887833818\n",
      "Epoch [26/300], Loss: 0.7247407341857091\n",
      "Epoch [27/300], Loss: 0.7168904572267971\n",
      "Epoch [28/300], Loss: 0.7173282679008401\n",
      "Epoch [29/300], Loss: 0.7094489666430847\n",
      "Epoch [30/300], Loss: 0.7066681711265194\n",
      "Epoch [31/300], Loss: 0.6992660597385958\n",
      "Epoch [32/300], Loss: 0.6972277490493587\n",
      "Epoch [33/300], Loss: 0.6927395030055814\n",
      "Epoch [34/300], Loss: 0.6844892401981841\n",
      "Epoch [35/300], Loss: 0.6781600080716336\n",
      "Epoch [36/300], Loss: 0.679450091193704\n",
      "Epoch [37/300], Loss: 0.6788625897806319\n",
      "Epoch [38/300], Loss: 0.6754891434517662\n",
      "Epoch [39/300], Loss: 0.6683085605387797\n",
      "Epoch [40/300], Loss: 0.6628568060029193\n",
      "Epoch [41/300], Loss: 0.6514059953067614\n",
      "Epoch [42/300], Loss: 0.6576408316640903\n",
      "Epoch [43/300], Loss: 0.6638508222978133\n",
      "Epoch [44/300], Loss: 0.6556415758703066\n",
      "Epoch [45/300], Loss: 0.6514562579905591\n",
      "Epoch [46/300], Loss: 0.652840541223126\n",
      "Epoch [47/300], Loss: 0.6410902216458869\n",
      "Epoch [48/300], Loss: 0.6432873068944268\n",
      "Epoch [49/300], Loss: 0.6395878794857913\n",
      "Epoch [50/300], Loss: 0.6335004641653021\n",
      "Epoch [51/300], Loss: 0.6326757718611251\n",
      "Epoch [52/300], Loss: 0.6336191418530691\n",
      "Epoch [53/300], Loss: 0.6334024701277008\n",
      "Epoch [54/300], Loss: 0.6310901155176065\n",
      "Epoch [55/300], Loss: 0.6249732198693868\n",
      "Epoch [56/300], Loss: 0.6274996535933536\n",
      "Epoch [57/300], Loss: 0.6295683500940538\n",
      "Epoch [58/300], Loss: 0.6157975856910276\n",
      "Epoch [59/300], Loss: 0.6165142381740043\n",
      "Epoch [60/300], Loss: 0.6115335486352901\n",
      "Epoch [61/300], Loss: 0.6221632633139106\n",
      "Epoch [62/300], Loss: 0.6163273750592375\n",
      "Epoch [63/300], Loss: 0.6103530437363993\n",
      "Epoch [64/300], Loss: 0.6070351206395023\n",
      "Epoch [65/300], Loss: 0.6070355399490317\n",
      "Epoch [66/300], Loss: 0.6062182108002245\n",
      "Epoch [67/300], Loss: 0.5985321098047754\n",
      "Epoch [68/300], Loss: 0.6044870327462626\n",
      "Epoch [69/300], Loss: 0.6042655906866273\n",
      "Epoch [70/300], Loss: 0.6056770597158185\n",
      "Epoch [71/300], Loss: 0.6018257760407065\n",
      "Epoch [72/300], Loss: 0.598713409965453\n",
      "Epoch [73/300], Loss: 0.5918362275947391\n",
      "Epoch [74/300], Loss: 0.6007838530461197\n",
      "Epoch [75/300], Loss: 0.596173860990178\n",
      "Epoch [76/300], Loss: 0.5965789028293337\n",
      "Epoch [77/300], Loss: 0.5922917704219404\n",
      "Epoch [78/300], Loss: 0.5909127980813651\n",
      "Epoch [79/300], Loss: 0.5906980501111511\n",
      "Epoch [80/300], Loss: 0.5829368380786818\n",
      "Epoch [81/300], Loss: 0.5887782784069285\n",
      "Epoch [82/300], Loss: 0.5840880105943631\n",
      "Epoch [83/300], Loss: 0.5811932518735261\n",
      "Epoch [84/300], Loss: 0.5823343908962082\n",
      "Epoch [85/300], Loss: 0.5783844068836983\n",
      "Epoch [86/300], Loss: 0.5781552753485072\n",
      "Epoch [87/300], Loss: 0.5740734490058611\n",
      "Epoch [88/300], Loss: 0.5797328336540696\n",
      "Epoch [89/300], Loss: 0.5741570306479779\n",
      "Epoch [90/300], Loss: 0.5773371337053111\n",
      "Epoch [91/300], Loss: 0.5758898602727124\n",
      "Epoch [92/300], Loss: 0.5721462309131842\n",
      "Epoch [93/300], Loss: 0.5666116683188912\n",
      "Epoch [94/300], Loss: 0.5695180694389221\n",
      "Epoch [95/300], Loss: 0.5673742174644909\n",
      "Epoch [96/300], Loss: 0.566744999202621\n",
      "Epoch [97/300], Loss: 0.5646839360218219\n",
      "Epoch [98/300], Loss: 0.5670417073513846\n",
      "Epoch [99/300], Loss: 0.5656499251951952\n",
      "Epoch [100/300], Loss: 0.5536221233780122\n",
      "Epoch [101/300], Loss: 0.5608806093711682\n",
      "Epoch [102/300], Loss: 0.5475782566439465\n",
      "Epoch [103/300], Loss: 0.5534125319146135\n",
      "Epoch [104/300], Loss: 0.5569738909945159\n",
      "Epoch [105/300], Loss: 0.5535836185678802\n",
      "Epoch [106/300], Loss: 0.5508428172915792\n",
      "Epoch [107/300], Loss: 0.5535039154007612\n",
      "Epoch [108/300], Loss: 0.5549647897062704\n",
      "Epoch [109/300], Loss: 0.5474937513203877\n",
      "Epoch [110/300], Loss: 0.5487195800827898\n",
      "Epoch [111/300], Loss: 0.5390511348729243\n",
      "Epoch [112/300], Loss: 0.5488158002724428\n",
      "Epoch [113/300], Loss: 0.5453341599658627\n",
      "Epoch [114/300], Loss: 0.5399102036795957\n",
      "Epoch [115/300], Loss: 0.5430106073808487\n",
      "Epoch [116/300], Loss: 0.5392418909827461\n",
      "Epoch [117/300], Loss: 0.5414767946161883\n",
      "Epoch [118/300], Loss: 0.5412469153933208\n",
      "Epoch [119/300], Loss: 0.5471772087542602\n",
      "Epoch [120/300], Loss: 0.5471971578648328\n",
      "Epoch [121/300], Loss: 0.5379880913688094\n",
      "Epoch [122/300], Loss: 0.5423903415918045\n",
      "Epoch [123/300], Loss: 0.5377633310759159\n",
      "Epoch [124/300], Loss: 0.5456949328370106\n",
      "Epoch [125/300], Loss: 0.5337102902514855\n",
      "Epoch [126/300], Loss: 0.5393881731097351\n",
      "Epoch [127/300], Loss: 0.5269822409314573\n",
      "Epoch [128/300], Loss: 0.5313650875368996\n",
      "Epoch [129/300], Loss: 0.5273929779486888\n",
      "Epoch [130/300], Loss: 0.5345300692121696\n",
      "Epoch [131/300], Loss: 0.5253082302677662\n",
      "Epoch [132/300], Loss: 0.5273735515411248\n",
      "Epoch [133/300], Loss: 0.5299860271613311\n",
      "Epoch [134/300], Loss: 0.5339505915217997\n",
      "Epoch [135/300], Loss: 0.5285883271099662\n",
      "Epoch [136/300], Loss: 0.5270701253886723\n",
      "Epoch [137/300], Loss: 0.5240123255173569\n",
      "Epoch [138/300], Loss: 0.534600805443571\n",
      "Epoch [139/300], Loss: 0.5226735082428778\n",
      "Epoch [140/300], Loss: 0.5280722460287917\n",
      "Epoch [141/300], Loss: 0.5265698773438668\n",
      "Epoch [142/300], Loss: 0.5204909784752695\n",
      "Epoch [143/300], Loss: 0.5235737690611568\n",
      "Epoch [144/300], Loss: 0.5270930848577443\n",
      "Epoch [145/300], Loss: 0.5268745906746296\n",
      "Epoch [146/300], Loss: 0.5167072485856083\n",
      "Epoch [147/300], Loss: 0.5130066045600435\n",
      "Epoch [148/300], Loss: 0.523666110451874\n",
      "Epoch [149/300], Loss: 0.5213770485480728\n",
      "Epoch [150/300], Loss: 0.5272516076407774\n",
      "Epoch [151/300], Loss: 0.5115065698314201\n",
      "Epoch [152/300], Loss: 0.5205435035631175\n",
      "Epoch [153/300], Loss: 0.5178813245480932\n",
      "Epoch [154/300], Loss: 0.5194848817403969\n",
      "Epoch [155/300], Loss: 0.5153540655246476\n",
      "Epoch [156/300], Loss: 0.5170332186705316\n",
      "Epoch [157/300], Loss: 0.5155855896680251\n",
      "Epoch [158/300], Loss: 0.5133561242343215\n",
      "Epoch [159/300], Loss: 0.5160462913077201\n",
      "Epoch [160/300], Loss: 0.5075997332554034\n",
      "Epoch [161/300], Loss: 0.5136479325116138\n",
      "Epoch [162/300], Loss: 0.5230895367920246\n",
      "Epoch [163/300], Loss: 0.5117948453139771\n",
      "Epoch [164/300], Loss: 0.5138522433236127\n",
      "Epoch [165/300], Loss: 0.5137568955188212\n",
      "Epoch [166/300], Loss: 0.5073625621054788\n",
      "Epoch [167/300], Loss: 0.509818253671879\n",
      "Epoch [168/300], Loss: 0.5149365996613222\n",
      "Epoch [169/300], Loss: 0.5163416696517059\n",
      "Epoch [170/300], Loss: 0.5142665544662939\n",
      "Epoch [171/300], Loss: 0.5155311302112802\n",
      "Epoch [172/300], Loss: 0.5134993006887338\n",
      "Epoch [173/300], Loss: 0.515119466311334\n",
      "Epoch [174/300], Loss: 0.5070602378386366\n",
      "Epoch [175/300], Loss: 0.5146029191210751\n",
      "Epoch [176/300], Loss: 0.5076048902195432\n",
      "Epoch [177/300], Loss: 0.5058445369877169\n",
      "Epoch [178/300], Loss: 0.510854780502484\n",
      "Epoch [179/300], Loss: 0.5142700301335595\n",
      "Epoch [180/300], Loss: 0.5067418980438386\n",
      "Epoch [181/300], Loss: 0.5112229966370346\n",
      "Epoch [182/300], Loss: 0.5018336472608854\n",
      "Epoch [183/300], Loss: 0.5071964123098137\n",
      "Epoch [184/300], Loss: 0.5066789949450956\n",
      "Epoch [185/300], Loss: 0.5073513419693693\n",
      "Epoch [186/300], Loss: 0.5061602997486396\n",
      "Epoch [187/300], Loss: 0.5044496042268051\n",
      "Epoch [188/300], Loss: 0.5092355698118429\n",
      "Epoch [189/300], Loss: 0.5013240844849736\n",
      "Epoch [190/300], Loss: 0.5097137200634193\n",
      "Epoch [191/300], Loss: 0.5063145968615247\n",
      "Epoch [192/300], Loss: 0.5000813872651066\n",
      "Epoch [193/300], Loss: 0.5023434235311836\n",
      "Epoch [194/300], Loss: 0.502260448930361\n",
      "Epoch [195/300], Loss: 0.5011690132643866\n",
      "Epoch [196/300], Loss: 0.5072392011847338\n",
      "Epoch [197/300], Loss: 0.49361337841395525\n",
      "Epoch [198/300], Loss: 0.5108493842813365\n",
      "Epoch [199/300], Loss: 0.5063878100965639\n",
      "Epoch [200/300], Loss: 0.49977920484512356\n",
      "Epoch [201/300], Loss: 0.4989702366959408\n",
      "Epoch [202/300], Loss: 0.4994225729152065\n",
      "Epoch [203/300], Loss: 0.5022066361116021\n",
      "Epoch [204/300], Loss: 0.5027716300257331\n",
      "Epoch [205/300], Loss: 0.5001576167657552\n",
      "Epoch [206/300], Loss: 0.5028601809955009\n",
      "Epoch [207/300], Loss: 0.4973749537830767\n",
      "Epoch [208/300], Loss: 0.5068197630517318\n",
      "Epoch [209/300], Loss: 0.5031302745651711\n",
      "Epoch [210/300], Loss: 0.5039500418069113\n",
      "Epoch [211/300], Loss: 0.5022213317244254\n",
      "Epoch [212/300], Loss: 0.49687865119227365\n",
      "Epoch [213/300], Loss: 0.5056957808011175\n",
      "Epoch [214/300], Loss: 0.4978316556995787\n",
      "Epoch [215/300], Loss: 0.49319238834978674\n",
      "Epoch [216/300], Loss: 0.4961188320270585\n",
      "Epoch [217/300], Loss: 0.4941524632484712\n",
      "Epoch [218/300], Loss: 0.4953453152457162\n",
      "Epoch [219/300], Loss: 0.5098152561946903\n",
      "Epoch [220/300], Loss: 0.4912180346829812\n",
      "Epoch [221/300], Loss: 0.4972721422686601\n",
      "Epoch [222/300], Loss: 0.49924174126456766\n",
      "Epoch [223/300], Loss: 0.49561549351572076\n",
      "Epoch [224/300], Loss: 0.5000362148141617\n",
      "Epoch [225/300], Loss: 0.4902608814408712\n",
      "Epoch [226/300], Loss: 0.49240339061488275\n",
      "Epoch [227/300], Loss: 0.49203340474830565\n",
      "Epoch [228/300], Loss: 0.4934691218159083\n",
      "Epoch [229/300], Loss: 0.4955240428790717\n",
      "Epoch [230/300], Loss: 0.4973837435245514\n",
      "Epoch [231/300], Loss: 0.5037773507635307\n",
      "Epoch [232/300], Loss: 0.49790073150907027\n",
      "Epoch [233/300], Loss: 0.49307984940688626\n",
      "Epoch [234/300], Loss: 0.49850578851940686\n",
      "Epoch [235/300], Loss: 0.5004954396382623\n",
      "Epoch [236/300], Loss: 0.4970123247074349\n",
      "Epoch [237/300], Loss: 0.48747077698597824\n",
      "Epoch [238/300], Loss: 0.4954121454483103\n",
      "Epoch [239/300], Loss: 0.48735415712570596\n",
      "Epoch [240/300], Loss: 0.493224838353179\n",
      "Epoch [241/300], Loss: 0.4936942169180764\n",
      "Epoch [242/300], Loss: 0.4972019778455005\n",
      "Epoch [243/300], Loss: 0.4951713103658098\n",
      "Epoch [244/300], Loss: 0.48797247265382193\n",
      "Epoch [245/300], Loss: 0.48966033827237154\n",
      "Epoch [246/300], Loss: 0.48308718759004415\n",
      "Epoch [247/300], Loss: 0.4960173148175944\n",
      "Epoch [248/300], Loss: 0.4926816101764779\n",
      "Epoch [249/300], Loss: 0.49012862467933493\n",
      "Epoch [250/300], Loss: 0.4837603560455925\n",
      "Epoch [251/300], Loss: 0.489465090662927\n",
      "Epoch [252/300], Loss: 0.49380792126707407\n",
      "Epoch [253/300], Loss: 0.48760667168880667\n",
      "Epoch [254/300], Loss: 0.49296562796663446\n",
      "Epoch [255/300], Loss: 0.4849236626606768\n",
      "Epoch [256/300], Loss: 0.4904125594269589\n",
      "Epoch [257/300], Loss: 0.49452560246371857\n",
      "Epoch [258/300], Loss: 0.49637489617251984\n",
      "Epoch [259/300], Loss: 0.49004319268266866\n",
      "Epoch [260/300], Loss: 0.48651815600254955\n",
      "Epoch [261/300], Loss: 0.48989119939029674\n",
      "Epoch [262/300], Loss: 0.4932134240637045\n",
      "Epoch [263/300], Loss: 0.48716464091825973\n",
      "Epoch [264/300], Loss: 0.4819264162493789\n",
      "Epoch [265/300], Loss: 0.4867902200316529\n",
      "Epoch [266/300], Loss: 0.493073194437777\n",
      "Epoch [267/300], Loss: 0.4862962902506904\n",
      "Epoch [268/300], Loss: 0.4929723455511091\n",
      "Epoch [269/300], Loss: 0.5017783637051387\n",
      "Epoch [270/300], Loss: 0.4829537605347536\n",
      "Epoch [271/300], Loss: 0.4893307820191164\n",
      "Epoch [272/300], Loss: 0.48193434176161465\n",
      "Epoch [273/300], Loss: 0.4915759710552138\n",
      "Epoch [274/300], Loss: 0.4901301928645814\n",
      "Epoch [275/300], Loss: 0.4826210779340371\n",
      "Epoch [276/300], Loss: 0.4922567173419401\n",
      "Epoch [277/300], Loss: 0.48680279392491826\n",
      "Epoch [278/300], Loss: 0.4810943638767733\n",
      "Epoch [279/300], Loss: 0.4839855726722561\n",
      "Epoch [280/300], Loss: 0.4843228401430428\n",
      "Epoch [281/300], Loss: 0.4916125584555709\n",
      "Epoch [282/300], Loss: 0.4909255916009779\n",
      "Epoch [283/300], Loss: 0.4826370354008187\n",
      "Epoch [284/300], Loss: 0.48577714749538076\n",
      "Epoch [285/300], Loss: 0.48034315138979033\n",
      "Epoch [286/300], Loss: 0.4858440834638255\n",
      "Epoch [287/300], Loss: 0.48956101433471644\n",
      "Epoch [288/300], Loss: 0.47730622381505455\n",
      "Epoch [289/300], Loss: 0.4886062704312527\n",
      "Epoch [290/300], Loss: 0.47785144663222917\n",
      "Epoch [291/300], Loss: 0.4835307046275614\n",
      "Epoch [292/300], Loss: 0.4805428700907456\n",
      "Epoch [293/300], Loss: 0.4868439841072273\n",
      "Epoch [294/300], Loss: 0.48127485017108795\n",
      "Epoch [295/300], Loss: 0.48216917653522834\n",
      "Epoch [296/300], Loss: 0.48054722991898235\n",
      "Epoch [297/300], Loss: 0.4805623457559844\n",
      "Epoch [298/300], Loss: 0.4789733171767896\n",
      "Epoch [299/300], Loss: 0.4897330502510223\n",
      "Epoch [300/300], Loss: 0.47919654991010874\n",
      "Training completed in: 1540.54 seconds\n",
      "Accuracy of the model on the 10000 test images: 72.36%\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# CIFAR10 Dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "# CNN Architecture\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(8 * 8 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader)}')\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f'Training completed in: {training_time:.2f} seconds')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the 10000 test images: {100 * correct / total}%')\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'cifar10_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e1c992-b181-481b-bdd2-fc59a19ac85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/300], Loss: 1.3799286731673628\n",
      "Epoch [2/300], Loss: 1.0487423141289245\n",
      "Epoch [3/300], Loss: 0.8831510139853144\n",
      "Epoch [4/300], Loss: 0.7698540664694803\n",
      "Epoch [5/300], Loss: 0.6616150987575121\n",
      "Epoch [6/300], Loss: 0.580741146031548\n",
      "Epoch [7/300], Loss: 0.4992279327281601\n",
      "Epoch [8/300], Loss: 0.4177770873393549\n",
      "Epoch [9/300], Loss: 0.3511101286810682\n",
      "Epoch [10/300], Loss: 0.30481810277075416\n",
      "Epoch [11/300], Loss: 0.26484668653105836\n",
      "Epoch [12/300], Loss: 0.22321723153828965\n",
      "Epoch [13/300], Loss: 0.20123543451204323\n",
      "Epoch [14/300], Loss: 0.18112143330142627\n",
      "Epoch [15/300], Loss: 0.16103756136220435\n",
      "Epoch [16/300], Loss: 0.14626412157474272\n",
      "Epoch [17/300], Loss: 0.13874410177626267\n",
      "Epoch [18/300], Loss: 0.12919597764073126\n",
      "Epoch [19/300], Loss: 0.12151677928426687\n",
      "Epoch [20/300], Loss: 0.12052348753451692\n",
      "Epoch [21/300], Loss: 0.11355746538399736\n",
      "Epoch [22/300], Loss: 0.10841676740028212\n",
      "Epoch [23/300], Loss: 0.09823974776689125\n",
      "Epoch [24/300], Loss: 0.10172350064415456\n",
      "Epoch [25/300], Loss: 0.09544216290287807\n",
      "Epoch [26/300], Loss: 0.09080993252165635\n",
      "Epoch [27/300], Loss: 0.09289391540810275\n",
      "Epoch [28/300], Loss: 0.08844913635164728\n",
      "Epoch [29/300], Loss: 0.09053647979531827\n",
      "Epoch [30/300], Loss: 0.07930196307437576\n",
      "Epoch [31/300], Loss: 0.08341087849901231\n",
      "Epoch [32/300], Loss: 0.08185872518221664\n",
      "Epoch [33/300], Loss: 0.07568685470334709\n",
      "Epoch [34/300], Loss: 0.0770823078973176\n",
      "Epoch [35/300], Loss: 0.07580233979827303\n",
      "Epoch [36/300], Loss: 0.0753841727633801\n",
      "Epoch [37/300], Loss: 0.07437769124460647\n",
      "Epoch [38/300], Loss: 0.07210669985465\n",
      "Epoch [39/300], Loss: 0.07000783019129882\n",
      "Epoch [40/300], Loss: 0.06866138227417341\n",
      "Epoch [41/300], Loss: 0.06714420496960126\n",
      "Epoch [42/300], Loss: 0.06882033944415772\n",
      "Epoch [43/300], Loss: 0.07178993538841415\n",
      "Epoch [44/300], Loss: 0.0690060756038255\n",
      "Epoch [45/300], Loss: 0.06584701215719704\n",
      "Epoch [46/300], Loss: 0.06166341938935887\n",
      "Epoch [47/300], Loss: 0.06417783330697233\n",
      "Epoch [48/300], Loss: 0.06707862264398114\n",
      "Epoch [49/300], Loss: 0.06295143605907784\n",
      "Epoch [50/300], Loss: 0.06305168866587188\n",
      "Epoch [51/300], Loss: 0.04111781557354971\n",
      "Epoch [52/300], Loss: 0.03292614579810511\n",
      "Epoch [53/300], Loss: 0.027370549578819892\n",
      "Epoch [54/300], Loss: 0.02758055529735811\n",
      "Epoch [55/300], Loss: 0.0244406156165673\n",
      "Epoch [56/300], Loss: 0.02161334767756636\n",
      "Epoch [57/300], Loss: 0.021788052907285027\n",
      "Epoch [58/300], Loss: 0.020502007783145246\n",
      "Epoch [59/300], Loss: 0.0203304293284388\n",
      "Epoch [60/300], Loss: 0.01976051597434389\n",
      "Epoch [61/300], Loss: 0.019353448905174615\n",
      "Epoch [62/300], Loss: 0.016791735842461932\n",
      "Epoch [63/300], Loss: 0.018069029459372506\n",
      "Epoch [64/300], Loss: 0.016731386555268256\n",
      "Epoch [65/300], Loss: 0.01748956174201444\n",
      "Epoch [66/300], Loss: 0.017009804268246113\n",
      "Epoch [67/300], Loss: 0.017408890865535934\n",
      "Epoch [68/300], Loss: 0.015421888530504463\n",
      "Epoch [69/300], Loss: 0.014011285518465177\n",
      "Epoch [70/300], Loss: 0.015165334662703601\n",
      "Epoch [71/300], Loss: 0.013966642937067982\n",
      "Epoch [72/300], Loss: 0.013622287426517842\n",
      "Epoch [73/300], Loss: 0.013605867635899061\n",
      "Epoch [74/300], Loss: 0.014353943521828602\n",
      "Epoch [75/300], Loss: 0.013973136394358505\n",
      "Epoch [76/300], Loss: 0.012896481228520726\n",
      "Epoch [77/300], Loss: 0.012499416846057872\n",
      "Epoch [78/300], Loss: 0.0118087601390622\n",
      "Epoch [79/300], Loss: 0.011859913830004652\n",
      "Epoch [80/300], Loss: 0.012610779205799255\n",
      "Epoch [81/300], Loss: 0.011955490507318846\n",
      "Epoch [82/300], Loss: 0.01181545360025752\n",
      "Epoch [83/300], Loss: 0.011959188073501943\n",
      "Epoch [84/300], Loss: 0.01201663710310331\n",
      "Epoch [85/300], Loss: 0.011463339697769689\n",
      "Epoch [86/300], Loss: 0.011506139528532714\n",
      "Epoch [87/300], Loss: 0.0114052815057928\n",
      "Epoch [88/300], Loss: 0.013206778794450834\n",
      "Epoch [89/300], Loss: 0.010267423875534625\n",
      "Epoch [90/300], Loss: 0.00982441974010633\n",
      "Epoch [91/300], Loss: 0.011509617171286013\n",
      "Epoch [92/300], Loss: 0.010041778205115053\n",
      "Epoch [93/300], Loss: 0.009722775518251082\n",
      "Epoch [94/300], Loss: 0.010203319585041316\n",
      "Epoch [95/300], Loss: 0.009562893393401847\n",
      "Epoch [96/300], Loss: 0.009575689778379772\n",
      "Epoch [97/300], Loss: 0.009765910982723584\n",
      "Epoch [98/300], Loss: 0.010178599032023183\n",
      "Epoch [99/300], Loss: 0.009413240061801336\n",
      "Epoch [100/300], Loss: 0.00996550085628048\n",
      "Epoch [101/300], Loss: 0.009142241271598565\n",
      "Epoch [102/300], Loss: 0.008033859162398464\n",
      "Epoch [103/300], Loss: 0.008909639216425931\n",
      "Epoch [104/300], Loss: 0.00797470499847151\n",
      "Epoch [105/300], Loss: 0.00915858701079586\n",
      "Epoch [106/300], Loss: 0.008698784601529274\n",
      "Epoch [107/300], Loss: 0.007219733110607585\n",
      "Epoch [108/300], Loss: 0.007414113523562431\n",
      "Epoch [109/300], Loss: 0.008542297095955942\n",
      "Epoch [110/300], Loss: 0.00709960880734579\n",
      "Epoch [111/300], Loss: 0.008715490270365812\n",
      "Epoch [112/300], Loss: 0.009489089885042965\n",
      "Epoch [113/300], Loss: 0.007213060732197274\n",
      "Epoch [114/300], Loss: 0.00792701189856395\n",
      "Epoch [115/300], Loss: 0.008553300935374883\n",
      "Epoch [116/300], Loss: 0.008861197700874065\n",
      "Epoch [117/300], Loss: 0.007002606983189387\n",
      "Epoch [118/300], Loss: 0.00735516471745413\n",
      "Epoch [119/300], Loss: 0.00718125260358824\n",
      "Epoch [120/300], Loss: 0.007710389472851458\n",
      "Epoch [121/300], Loss: 0.007489549676123102\n",
      "Epoch [122/300], Loss: 0.0069797598397182995\n",
      "Epoch [123/300], Loss: 0.007541031309436528\n",
      "Epoch [124/300], Loss: 0.007955864624446616\n",
      "Epoch [125/300], Loss: 0.007195955544919767\n",
      "Epoch [126/300], Loss: 0.0061893454464652655\n",
      "Epoch [127/300], Loss: 0.006692337253323907\n",
      "Epoch [128/300], Loss: 0.006884957096822884\n",
      "Epoch [129/300], Loss: 0.007791122668446578\n",
      "Epoch [130/300], Loss: 0.007230076451471452\n",
      "Epoch [131/300], Loss: 0.0070666851585402205\n",
      "Epoch [132/300], Loss: 0.007015321767576934\n",
      "Epoch [133/300], Loss: 0.007481752875316983\n",
      "Epoch [134/300], Loss: 0.008074270901949052\n",
      "Epoch [135/300], Loss: 0.006059200771640072\n",
      "Epoch [136/300], Loss: 0.006538030546149025\n",
      "Epoch [137/300], Loss: 0.0072028439076941295\n",
      "Epoch [138/300], Loss: 0.007844903708800026\n",
      "Epoch [139/300], Loss: 0.0072199091658560685\n",
      "Epoch [140/300], Loss: 0.00676525772868982\n",
      "Epoch [141/300], Loss: 0.007652455638341434\n",
      "Epoch [142/300], Loss: 0.0065655474749910635\n",
      "Epoch [143/300], Loss: 0.007023057433516931\n",
      "Epoch [144/300], Loss: 0.006646972561195252\n",
      "Epoch [145/300], Loss: 0.007108865130953687\n",
      "Epoch [146/300], Loss: 0.0066558389869087456\n",
      "Epoch [147/300], Loss: 0.007092758827983304\n",
      "Epoch [148/300], Loss: 0.006685045758804754\n",
      "Epoch [149/300], Loss: 0.007140833040317663\n",
      "Epoch [150/300], Loss: 0.006668663868332839\n",
      "Epoch [151/300], Loss: 0.0069035528968457525\n",
      "Epoch [152/300], Loss: 0.007426248065403203\n",
      "Epoch [153/300], Loss: 0.007241362090701299\n",
      "Epoch [154/300], Loss: 0.006554137636690646\n",
      "Epoch [155/300], Loss: 0.006551814425136427\n",
      "Epoch [156/300], Loss: 0.00801288054974945\n",
      "Epoch [157/300], Loss: 0.00668495360498681\n",
      "Epoch [158/300], Loss: 0.007183979984725375\n",
      "Epoch [159/300], Loss: 0.006236417753779141\n",
      "Epoch [160/300], Loss: 0.006322422424507568\n",
      "Epoch [161/300], Loss: 0.007227365115700323\n",
      "Epoch [162/300], Loss: 0.006560948912685206\n",
      "Epoch [163/300], Loss: 0.006514796701348041\n",
      "Epoch [164/300], Loss: 0.007365399170095277\n",
      "Epoch [165/300], Loss: 0.007572690258398081\n",
      "Epoch [166/300], Loss: 0.006701128059507484\n",
      "Epoch [167/300], Loss: 0.006168513843441939\n",
      "Epoch [168/300], Loss: 0.007007686604681374\n",
      "Epoch [169/300], Loss: 0.00645791121643594\n",
      "Epoch [170/300], Loss: 0.0064139056312458595\n",
      "Epoch [171/300], Loss: 0.006720634546521527\n",
      "Epoch [172/300], Loss: 0.007090281211006482\n",
      "Epoch [173/300], Loss: 0.006093988896828726\n",
      "Epoch [174/300], Loss: 0.007051509774952193\n",
      "Epoch [175/300], Loss: 0.0077402800473946095\n",
      "Epoch [176/300], Loss: 0.007028918673434412\n",
      "Epoch [177/300], Loss: 0.007073356235242637\n",
      "Epoch [178/300], Loss: 0.007994597953984805\n",
      "Epoch [179/300], Loss: 0.007353931322427052\n",
      "Epoch [180/300], Loss: 0.007658688658776948\n",
      "Epoch [181/300], Loss: 0.0066326826410677735\n",
      "Epoch [182/300], Loss: 0.0064557169485465644\n",
      "Epoch [183/300], Loss: 0.005881516607668813\n",
      "Epoch [184/300], Loss: 0.006900500981649265\n",
      "Epoch [185/300], Loss: 0.0070599919537526245\n",
      "Epoch [186/300], Loss: 0.0072694666662236765\n",
      "Epoch [187/300], Loss: 0.006761134181426519\n",
      "Epoch [188/300], Loss: 0.006607547914490218\n",
      "Epoch [189/300], Loss: 0.006065960681718557\n",
      "Epoch [190/300], Loss: 0.006574934161484927\n",
      "Epoch [191/300], Loss: 0.006679444285609838\n",
      "Epoch [192/300], Loss: 0.007159690502757097\n",
      "Epoch [193/300], Loss: 0.00626165572318065\n",
      "Epoch [194/300], Loss: 0.007629194809004779\n",
      "Epoch [195/300], Loss: 0.006746567259340182\n",
      "Epoch [196/300], Loss: 0.008014494172104484\n",
      "Epoch [197/300], Loss: 0.007235507866667817\n",
      "Epoch [198/300], Loss: 0.00731243160517548\n",
      "Epoch [199/300], Loss: 0.0070704952744461\n",
      "Epoch [200/300], Loss: 0.007169945278357419\n",
      "Epoch [201/300], Loss: 0.007874528786924946\n",
      "Epoch [202/300], Loss: 0.006309790508773968\n",
      "Epoch [203/300], Loss: 0.0064802998386602135\n",
      "Epoch [204/300], Loss: 0.007062463665350109\n",
      "Epoch [205/300], Loss: 0.008084556932234779\n",
      "Epoch [206/300], Loss: 0.007360243096041357\n",
      "Epoch [207/300], Loss: 0.007042883060481923\n",
      "Epoch [208/300], Loss: 0.006263321443327495\n",
      "Epoch [209/300], Loss: 0.0070039705966439695\n",
      "Epoch [210/300], Loss: 0.007105574769246609\n",
      "Epoch [211/300], Loss: 0.007188551790078583\n",
      "Epoch [212/300], Loss: 0.007540990415093539\n",
      "Epoch [213/300], Loss: 0.007296621317134413\n",
      "Epoch [214/300], Loss: 0.006304880032492111\n",
      "Epoch [215/300], Loss: 0.006455368994761382\n",
      "Epoch [216/300], Loss: 0.006727350298869078\n",
      "Epoch [217/300], Loss: 0.007502199742523834\n",
      "Epoch [218/300], Loss: 0.007081919614120823\n",
      "Epoch [219/300], Loss: 0.007028109592186031\n",
      "Epoch [220/300], Loss: 0.006457040058996747\n",
      "Epoch [221/300], Loss: 0.0066793964746410545\n",
      "Epoch [222/300], Loss: 0.007463221967606174\n",
      "Epoch [223/300], Loss: 0.00652549462988401\n",
      "Epoch [224/300], Loss: 0.007976624532602727\n",
      "Epoch [225/300], Loss: 0.0077465030262270545\n",
      "Epoch [226/300], Loss: 0.008270495796583288\n",
      "Epoch [227/300], Loss: 0.0072394736239786645\n",
      "Epoch [228/300], Loss: 0.0062533966706270145\n",
      "Epoch [229/300], Loss: 0.006277826931470496\n",
      "Epoch [230/300], Loss: 0.007123128066489668\n",
      "Epoch [231/300], Loss: 0.006372984942256311\n",
      "Epoch [232/300], Loss: 0.007031566515217161\n",
      "Epoch [233/300], Loss: 0.00683749443196389\n",
      "Epoch [234/300], Loss: 0.006534227309688983\n",
      "Epoch [235/300], Loss: 0.006654117008685456\n",
      "Epoch [236/300], Loss: 0.00722367254316883\n",
      "Epoch [237/300], Loss: 0.00607754097825102\n",
      "Epoch [238/300], Loss: 0.0074377097027531714\n",
      "Epoch [239/300], Loss: 0.007717717795030159\n",
      "Epoch [240/300], Loss: 0.007464106038883876\n",
      "Epoch [241/300], Loss: 0.006680562993680673\n",
      "Epoch [242/300], Loss: 0.006416792366796595\n",
      "Epoch [243/300], Loss: 0.005822465749313969\n",
      "Epoch [244/300], Loss: 0.007433866549422011\n",
      "Epoch [245/300], Loss: 0.006114896577413735\n",
      "Epoch [246/300], Loss: 0.00798256853428643\n",
      "Epoch [247/300], Loss: 0.006819896039340402\n",
      "Epoch [248/300], Loss: 0.007464002308142765\n",
      "Epoch [249/300], Loss: 0.007231720543676116\n",
      "Epoch [250/300], Loss: 0.0065112180669632405\n",
      "Epoch [251/300], Loss: 0.006966286877174969\n",
      "Epoch [252/300], Loss: 0.007218441390015585\n",
      "Epoch [253/300], Loss: 0.007449374236452305\n",
      "Epoch [254/300], Loss: 0.006520967027815559\n",
      "Epoch [255/300], Loss: 0.006634846293513933\n",
      "Epoch [256/300], Loss: 0.006796018617777595\n",
      "Epoch [257/300], Loss: 0.007290931566215842\n",
      "Epoch [258/300], Loss: 0.005476310456176396\n",
      "Epoch [259/300], Loss: 0.007448138254236125\n",
      "Epoch [260/300], Loss: 0.0063181617532444695\n",
      "Epoch [261/300], Loss: 0.008276563841918641\n",
      "Epoch [262/300], Loss: 0.007658470405356201\n",
      "Epoch [263/300], Loss: 0.006950481899578096\n",
      "Epoch [264/300], Loss: 0.007226674245846699\n",
      "Epoch [265/300], Loss: 0.007085796632363921\n",
      "Epoch [266/300], Loss: 0.006575677159425143\n",
      "Epoch [267/300], Loss: 0.007317606885524471\n",
      "Epoch [268/300], Loss: 0.007340503938715248\n",
      "Epoch [269/300], Loss: 0.0072339208544436795\n",
      "Epoch [270/300], Loss: 0.00698554842158809\n",
      "Epoch [271/300], Loss: 0.006860577268525958\n",
      "Epoch [272/300], Loss: 0.007030169497413175\n",
      "Epoch [273/300], Loss: 0.007421900153807972\n",
      "Epoch [274/300], Loss: 0.005788225065166478\n",
      "Epoch [275/300], Loss: 0.006822024854054898\n",
      "Epoch [276/300], Loss: 0.006975549084129636\n",
      "Epoch [277/300], Loss: 0.00714511038048331\n",
      "Epoch [278/300], Loss: 0.006139449797605094\n",
      "Epoch [279/300], Loss: 0.006681090172575525\n",
      "Epoch [280/300], Loss: 0.00744230324602531\n",
      "Epoch [281/300], Loss: 0.00809284546257704\n",
      "Epoch [282/300], Loss: 0.0057101780286682845\n",
      "Epoch [283/300], Loss: 0.007017819529168232\n",
      "Epoch [284/300], Loss: 0.007090605060344257\n",
      "Epoch [285/300], Loss: 0.007375878360940624\n",
      "Epoch [286/300], Loss: 0.006146872143634497\n",
      "Epoch [287/300], Loss: 0.007250487712709724\n",
      "Epoch [288/300], Loss: 0.0064930604918382925\n",
      "Epoch [289/300], Loss: 0.008141018249734027\n",
      "Epoch [290/300], Loss: 0.0066487114344154245\n",
      "Epoch [291/300], Loss: 0.006545150004651236\n",
      "Epoch [292/300], Loss: 0.006204621938248987\n",
      "Epoch [293/300], Loss: 0.007009030102997485\n",
      "Epoch [294/300], Loss: 0.0068748356688700975\n",
      "Epoch [295/300], Loss: 0.007408571903548582\n",
      "Epoch [296/300], Loss: 0.007664219860959312\n",
      "Epoch [297/300], Loss: 0.008084612200036645\n",
      "Epoch [298/300], Loss: 0.006522326684935623\n",
      "Epoch [299/300], Loss: 0.006093482482139869\n",
      "Epoch [300/300], Loss: 0.00653127594457944\n",
      "Training completed in: 2335.66 seconds\n",
      "Accuracy on test images: 78.37%\n"
     ]
    }
   ],
   "source": [
    "# part 1b\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 300\n",
    "batch_size = 128  # Adjust based on GPU capability\n",
    "learning_rate = 0.001\n",
    "num_workers = 4   # Based on system's capability\n",
    "lr_step_size = 50  # Learning rate step size\n",
    "lr_gamma = 0.1     # Learning rate decay factor\n",
    "\n",
    "# CIFAR10 Dataset with efficient data loading\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=num_workers)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Simplified CNN Architecture\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(8 * 8 * 64, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layers(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc_layers(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n",
    "\n",
    "# Initialize the gradient scaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader)}')\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f'Training completed in: {training_time:.2f} seconds')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on test images: {accuracy}%')\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'cifar10_extended_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/300], Loss: 1.2426466781007663\n",
      "Epoch [2/300], Loss: 0.7660231165721288\n",
      "Epoch [3/300], Loss: 0.5638005886503192\n",
      "Epoch [4/300], Loss: 0.42534488350000527\n",
      "Epoch [5/300], Loss: 0.3113696592123917\n",
      "Epoch [6/300], Loss: 0.20641383223349938\n",
      "Epoch [7/300], Loss: 0.1419284339265331\n",
      "Epoch [8/300], Loss: 0.09960545654363377\n",
      "Epoch [9/300], Loss: 0.0766152562835561\n",
      "Epoch [10/300], Loss: 0.07165640188753843\n",
      "Epoch [11/300], Loss: 0.05505694715958803\n",
      "Epoch [12/300], Loss: 0.056522922242915406\n",
      "Epoch [13/300], Loss: 0.044024212211888535\n",
      "Epoch [14/300], Loss: 0.04797049951733655\n",
      "Epoch [15/300], Loss: 0.030160778478640096\n",
      "Epoch [16/300], Loss: 0.04497442332327442\n",
      "Epoch [17/300], Loss: 0.02818851375567835\n",
      "Epoch [18/300], Loss: 0.033255965898499426\n",
      "Epoch [19/300], Loss: 0.029091637036801836\n",
      "Epoch [20/300], Loss: 0.03038401165287114\n",
      "Epoch [21/300], Loss: 0.027612376682352526\n",
      "Epoch [22/300], Loss: 0.028150657035151076\n",
      "Epoch [23/300], Loss: 0.022332393547312816\n",
      "Epoch [24/300], Loss: 0.02361325815838748\n",
      "Epoch [25/300], Loss: 0.029367643644089933\n",
      "Epoch [26/300], Loss: 0.01633184861603623\n",
      "Epoch [27/300], Loss: 0.025181661156732114\n",
      "Epoch [28/300], Loss: 0.02025482403433389\n",
      "Epoch [29/300], Loss: 0.020215466009181105\n",
      "Epoch [30/300], Loss: 0.018009957999915935\n",
      "Epoch [31/300], Loss: 0.020280752051573293\n",
      "Epoch [32/300], Loss: 0.016465531299974103\n",
      "Epoch [33/300], Loss: 0.01656781451810603\n",
      "Epoch [34/300], Loss: 0.017854240353983204\n",
      "Epoch [35/300], Loss: 0.01526455977660901\n",
      "Epoch [36/300], Loss: 0.00861133187555957\n",
      "Epoch [37/300], Loss: 0.020264272269539554\n",
      "Epoch [38/300], Loss: 0.014479262845949782\n",
      "Epoch [39/300], Loss: 0.014740292153883484\n",
      "Epoch [40/300], Loss: 0.010307024265768721\n",
      "Epoch [41/300], Loss: 0.01323648877439464\n",
      "Epoch [42/300], Loss: 0.014575877417087504\n",
      "Epoch [43/300], Loss: 0.013341203180971206\n",
      "Epoch [44/300], Loss: 0.014057227935580187\n",
      "Epoch [45/300], Loss: 0.01232599902143467\n",
      "Epoch [46/300], Loss: 0.013828911412282653\n",
      "Epoch [47/300], Loss: 0.010439358913556223\n",
      "Epoch [48/300], Loss: 0.011304014838546688\n",
      "Epoch [49/300], Loss: 0.008985812126518474\n",
      "Epoch [50/300], Loss: 0.008901353826878242\n",
      "Epoch [51/300], Loss: 0.012075508434977763\n",
      "Epoch [52/300], Loss: 0.013750411127066842\n",
      "Epoch [53/300], Loss: 0.009307071359013402\n",
      "Epoch [54/300], Loss: 0.009481971450944647\n",
      "Epoch [55/300], Loss: 0.009320098440266078\n",
      "Epoch [56/300], Loss: 0.009427192248678735\n",
      "Epoch [57/300], Loss: 0.013060296670681896\n",
      "Epoch [58/300], Loss: 0.011224109924747292\n",
      "Epoch [59/300], Loss: 0.0051916587342074145\n",
      "Epoch [60/300], Loss: 0.009407938806943445\n",
      "Epoch [61/300], Loss: 0.00832738591793539\n",
      "Epoch [62/300], Loss: 0.01015548172335679\n",
      "Epoch [63/300], Loss: 0.007757100290684525\n",
      "Epoch [64/300], Loss: 0.008467631553195713\n",
      "Epoch [65/300], Loss: 0.009252377417867086\n",
      "Epoch [66/300], Loss: 0.005984227122886485\n",
      "Epoch [67/300], Loss: 0.009829998677604266\n",
      "Epoch [68/300], Loss: 0.009392027968667727\n",
      "Epoch [69/300], Loss: 0.008321932874110653\n",
      "Epoch [70/300], Loss: 0.006729990690489924\n",
      "Epoch [71/300], Loss: 0.006286663791884992\n",
      "Epoch [72/300], Loss: 0.008584682654006234\n",
      "Epoch [73/300], Loss: 0.00866937741354477\n",
      "Epoch [74/300], Loss: 0.005765777813326342\n",
      "Epoch [75/300], Loss: 0.006681915288122454\n",
      "Epoch [76/300], Loss: 0.007830057351830918\n",
      "Epoch [77/300], Loss: 0.0063257795244939085\n",
      "Epoch [78/300], Loss: 0.005369214326845492\n",
      "Epoch [79/300], Loss: 0.010371203069718019\n",
      "Epoch [80/300], Loss: 0.005597297368983287\n",
      "Epoch [81/300], Loss: 0.005211514027409615\n",
      "Epoch [82/300], Loss: 0.007252826389575861\n",
      "Epoch [83/300], Loss: 0.007409712648505074\n",
      "Epoch [84/300], Loss: 0.004731030768093579\n",
      "Epoch [85/300], Loss: 0.006855440496974668\n",
      "Epoch [86/300], Loss: 0.0052686906900490885\n",
      "Epoch [87/300], Loss: 0.007076480462824746\n",
      "Epoch [88/300], Loss: 0.008334954557712024\n",
      "Epoch [89/300], Loss: 0.0040995413859300205\n",
      "Epoch [90/300], Loss: 0.006552974385581795\n",
      "Epoch [91/300], Loss: 0.006020627216116446\n",
      "Epoch [92/300], Loss: 0.004084843845236476\n",
      "Epoch [93/300], Loss: 0.006638762016183611\n",
      "Epoch [94/300], Loss: 0.0040775944181560475\n",
      "Epoch [95/300], Loss: 0.007917110573422512\n",
      "Epoch [96/300], Loss: 0.003275443125571603\n",
      "Epoch [97/300], Loss: 0.004707254600773055\n",
      "Epoch [98/300], Loss: 0.006595340781792132\n",
      "Epoch [99/300], Loss: 0.004082134519654931\n",
      "Epoch [100/300], Loss: 0.00533887526124935\n",
      "Epoch [101/300], Loss: 0.007182314541175408\n",
      "Epoch [102/300], Loss: 0.0033720587738343966\n",
      "Epoch [103/300], Loss: 0.0036912656594480153\n",
      "Epoch [104/300], Loss: 0.007109504568347656\n",
      "Epoch [105/300], Loss: 0.002908312140599192\n",
      "Epoch [106/300], Loss: 0.00543190094315388\n",
      "Epoch [107/300], Loss: 0.004075299972479334\n",
      "Epoch [108/300], Loss: 0.0036371238982554114\n",
      "Epoch [109/300], Loss: 0.004118045007641029\n",
      "Epoch [110/300], Loss: 0.008434814585965298\n",
      "Epoch [111/300], Loss: 0.0030425476143312712\n",
      "Epoch [112/300], Loss: 0.004896549924832647\n",
      "Epoch [113/300], Loss: 0.007341259401042801\n",
      "Epoch [114/300], Loss: 0.004452199057267886\n",
      "Epoch [115/300], Loss: 0.003439628884057815\n",
      "Epoch [116/300], Loss: 0.002775900473917176\n",
      "Epoch [117/300], Loss: 0.004517290343046869\n",
      "Epoch [118/300], Loss: 0.006731580550717053\n",
      "Epoch [119/300], Loss: 0.0035985336000246724\n",
      "Epoch [120/300], Loss: 0.005655587947346778\n",
      "Epoch [121/300], Loss: 0.0036257838571756144\n",
      "Epoch [122/300], Loss: 0.00449641656255217\n",
      "Epoch [123/300], Loss: 0.004053216611922221\n",
      "Epoch [124/300], Loss: 0.0022243042608093193\n",
      "Epoch [125/300], Loss: 0.0026136470002972967\n",
      "Epoch [126/300], Loss: 0.008126635419486189\n",
      "Epoch [127/300], Loss: 0.004870597318116938\n",
      "Epoch [128/300], Loss: 0.001363448566294233\n",
      "Epoch [129/300], Loss: 0.005152533956825029\n",
      "Epoch [130/300], Loss: 0.002493868627417622\n",
      "Epoch [131/300], Loss: 0.006684139368586385\n",
      "Epoch [132/300], Loss: 0.004206817944009708\n",
      "Epoch [133/300], Loss: 0.0021985040095781065\n",
      "Epoch [134/300], Loss: 0.0043215536267457375\n",
      "Epoch [135/300], Loss: 0.0026047847529881134\n",
      "Epoch [136/300], Loss: 0.0021137987258283528\n",
      "Epoch [137/300], Loss: 0.006796026901229709\n",
      "Epoch [138/300], Loss: 0.003102456253413654\n",
      "Epoch [139/300], Loss: 0.004521397116185111\n",
      "Epoch [140/300], Loss: 0.0035207557159559633\n",
      "Epoch [141/300], Loss: 0.0034937695380329358\n",
      "Epoch [142/300], Loss: 0.003889264068662536\n",
      "Epoch [143/300], Loss: 0.004253950569821025\n",
      "Epoch [144/300], Loss: 0.0032162812676720175\n",
      "Epoch [145/300], Loss: 0.003277133444027947\n",
      "Epoch [146/300], Loss: 0.005472311982862324\n",
      "Epoch [147/300], Loss: 0.0037840045209889205\n",
      "Epoch [148/300], Loss: 0.0011582474806838036\n",
      "Epoch [149/300], Loss: 0.004526868808648337\n",
      "Epoch [150/300], Loss: 0.004025349264121968\n",
      "Epoch [151/300], Loss: 0.0020752676838133115\n",
      "Epoch [152/300], Loss: 0.0025427886120621213\n",
      "Epoch [153/300], Loss: 0.006897350737223379\n",
      "Epoch [154/300], Loss: 0.0031715387714312546\n",
      "Epoch [155/300], Loss: 0.002030128805628697\n",
      "Epoch [156/300], Loss: 0.0023719738713889474\n",
      "Epoch [157/300], Loss: 0.003748284924142518\n",
      "Epoch [158/300], Loss: 0.0019442148052510337\n",
      "Epoch [159/300], Loss: 0.004799884793988479\n",
      "Epoch [160/300], Loss: 0.0028024959224398328\n",
      "Epoch [161/300], Loss: 0.0023921832453102708\n",
      "Epoch [162/300], Loss: 0.002879115655649571\n",
      "Epoch [163/300], Loss: 0.0042377998108854035\n",
      "Epoch [164/300], Loss: 0.0016042894659469655\n",
      "Epoch [165/300], Loss: 0.002433446060974799\n",
      "Epoch [166/300], Loss: 0.0065876176631456725\n",
      "Epoch [167/300], Loss: 0.0024276866534858163\n",
      "Epoch [168/300], Loss: 0.0029541704747825517\n",
      "Epoch [169/300], Loss: 0.0015508977876851624\n",
      "Epoch [170/300], Loss: 0.006349055949462344\n",
      "Epoch [171/300], Loss: 0.0035214481696145073\n",
      "Epoch [172/300], Loss: 0.0013253860168724084\n",
      "Epoch [173/300], Loss: 0.0011115355541003664\n",
      "Epoch [174/300], Loss: 0.00438231540378892\n",
      "Epoch [175/300], Loss: 0.004881592691321196\n",
      "Epoch [176/300], Loss: 0.002933334354492542\n",
      "Epoch [177/300], Loss: 0.0018114690098694527\n",
      "Epoch [178/300], Loss: 0.0015173932832065447\n",
      "Epoch [179/300], Loss: 0.005031229088389572\n",
      "Epoch [180/300], Loss: 0.0038563914914874647\n",
      "Epoch [181/300], Loss: 0.0017773522234310955\n",
      "Epoch [182/300], Loss: 0.002491526616202817\n",
      "Epoch [183/300], Loss: 0.003240929012484828\n",
      "Epoch [184/300], Loss: 0.0037116009491645493\n",
      "Epoch [185/300], Loss: 0.0019664005139645724\n",
      "Epoch [186/300], Loss: 0.001256352543045434\n",
      "Epoch [187/300], Loss: 0.002050141433861653\n",
      "Epoch [188/300], Loss: 0.00433727325666728\n",
      "Epoch [189/300], Loss: 0.003171443362788877\n",
      "Epoch [190/300], Loss: 0.002418550171077331\n",
      "Epoch [191/300], Loss: 0.005688692114954482\n",
      "Epoch [192/300], Loss: 0.0014721760261113296\n",
      "Epoch [193/300], Loss: 0.0011095836201147234\n",
      "Epoch [194/300], Loss: 0.003842047460137789\n",
      "Epoch [195/300], Loss: 0.00523018688150139\n",
      "Epoch [196/300], Loss: 0.0016954500398159442\n",
      "Epoch [197/300], Loss: 0.001521431033564247\n",
      "Epoch [198/300], Loss: 0.0042734543075966455\n",
      "Epoch [199/300], Loss: 0.0027064131241436745\n",
      "Epoch [200/300], Loss: 0.00356399413090657\n",
      "Epoch [201/300], Loss: 0.0026307012584935894\n",
      "Epoch [202/300], Loss: 0.0020532299250572267\n",
      "Epoch [203/300], Loss: 0.0011955700333605996\n",
      "Epoch [204/300], Loss: 0.002480570915669478\n",
      "Epoch [205/300], Loss: 0.003932761874638998\n",
      "Epoch [206/300], Loss: 0.00214129260495244\n",
      "Epoch [207/300], Loss: 0.001435698313651533\n",
      "Epoch [208/300], Loss: 0.0028796730404431273\n",
      "Epoch [209/300], Loss: 0.0034860339774296463\n",
      "Epoch [210/300], Loss: 0.002785930202914028\n",
      "Epoch [211/300], Loss: 0.0017203846680478132\n",
      "Epoch [212/300], Loss: 0.0010736056008255736\n",
      "Epoch [213/300], Loss: 0.0028969347209662683\n",
      "Epoch [214/300], Loss: 0.004226022460181517\n",
      "Epoch [215/300], Loss: 0.002361432082058164\n",
      "Epoch [216/300], Loss: 0.0018031331062924576\n",
      "Epoch [217/300], Loss: 0.0024822215900146692\n",
      "Epoch [218/300], Loss: 0.0038945145146970445\n",
      "Epoch [219/300], Loss: 0.001839931562538286\n",
      "Epoch [220/300], Loss: 0.0007249884780676317\n",
      "Epoch [221/300], Loss: 0.000880664987895269\n",
      "Epoch [222/300], Loss: 0.003836138760125378\n",
      "Epoch [223/300], Loss: 0.0041514359605422935\n",
      "Epoch [224/300], Loss: 0.0019187193653201538\n",
      "Epoch [225/300], Loss: 0.0015094169380970599\n",
      "Epoch [226/300], Loss: 0.0018754810108780674\n",
      "Epoch [227/300], Loss: 0.0020071298404993995\n",
      "Epoch [228/300], Loss: 0.0029789228514499697\n",
      "Epoch [229/300], Loss: 0.0039931248491668\n",
      "Epoch [230/300], Loss: 0.0014977166262608265\n",
      "Epoch [231/300], Loss: 0.0005024503793464528\n",
      "Epoch [232/300], Loss: 0.0015876841681955556\n",
      "Epoch [233/300], Loss: 0.0032826802457830047\n",
      "Epoch [234/300], Loss: 0.003856383454592553\n",
      "Epoch [235/300], Loss: 0.0018105834677722673\n",
      "Epoch [236/300], Loss: 0.0020161006911840603\n",
      "Epoch [237/300], Loss: 0.0020910014942891746\n",
      "Epoch [238/300], Loss: 0.002982868304793124\n",
      "Epoch [239/300], Loss: 0.0021023051886273803\n",
      "Epoch [240/300], Loss: 0.001441781879897619\n",
      "Epoch [241/300], Loss: 0.0018932581338391027\n",
      "Epoch [242/300], Loss: 0.0022119225462520894\n",
      "Epoch [243/300], Loss: 0.0012741762725276284\n",
      "Epoch [244/300], Loss: 0.002019650615955855\n",
      "Epoch [245/300], Loss: 0.0024128556748697445\n",
      "Epoch [246/300], Loss: 0.0009246391044832211\n",
      "Epoch [247/300], Loss: 0.005278074348449867\n",
      "Epoch [248/300], Loss: 0.002260413807572131\n",
      "Epoch [249/300], Loss: 0.0019959427925787684\n",
      "Epoch [250/300], Loss: 0.0012291729216024438\n",
      "Epoch [251/300], Loss: 0.0025809881112620814\n",
      "Epoch [252/300], Loss: 0.0014134768660093334\n",
      "Epoch [253/300], Loss: 0.005107581886762091\n",
      "Epoch [254/300], Loss: 0.0007590723899182946\n",
      "Epoch [255/300], Loss: 0.0009978315261005549\n",
      "Epoch [256/300], Loss: 0.002824826829415179\n",
      "Epoch [257/300], Loss: 0.0014681140699816637\n",
      "Epoch [258/300], Loss: 0.001890543995659602\n",
      "Epoch [259/300], Loss: 0.002506147317129417\n",
      "Epoch [260/300], Loss: 0.0021934329483153914\n",
      "Epoch [261/300], Loss: 0.0019486007615008049\n",
      "Epoch [262/300], Loss: 0.001195829024010257\n",
      "Epoch [263/300], Loss: 0.0010512126689685788\n",
      "Epoch [264/300], Loss: 0.003217910134148979\n",
      "Epoch [265/300], Loss: 0.0017038068074098538\n",
      "Epoch [266/300], Loss: 0.0022516305060023183\n",
      "Epoch [267/300], Loss: 0.0026686216083239315\n",
      "Epoch [268/300], Loss: 0.0013266676310233615\n",
      "Epoch [269/300], Loss: 0.00047179044555089916\n",
      "Epoch [270/300], Loss: 0.0013285026614620447\n",
      "Epoch [271/300], Loss: 0.003401390308476417\n",
      "Epoch [272/300], Loss: 0.00214878381399325\n",
      "Epoch [273/300], Loss: 0.0015190581528183793\n",
      "Epoch [274/300], Loss: 0.0024010051796514885\n",
      "Epoch [275/300], Loss: 0.0010100148234317423\n",
      "Epoch [276/300], Loss: 0.002282044746164704\n",
      "Epoch [277/300], Loss: 0.0008094901943089464\n",
      "Epoch [278/300], Loss: 0.0013324319404431771\n",
      "Epoch [279/300], Loss: 0.004449873739200557\n",
      "Epoch [280/300], Loss: 0.002491321542909809\n",
      "Epoch [281/300], Loss: 0.0028638993623206392\n",
      "Epoch [282/300], Loss: 0.0018759057403817541\n",
      "Epoch [283/300], Loss: 0.0006799347121783449\n",
      "Epoch [284/300], Loss: 0.0013217045586420711\n",
      "Epoch [285/300], Loss: 0.0023018386632713034\n",
      "Epoch [286/300], Loss: 0.003262032506384753\n",
      "Epoch [287/300], Loss: 0.0013460765653037642\n",
      "Epoch [288/300], Loss: 0.00141184730522002\n",
      "Epoch [289/300], Loss: 0.0014529081413180657\n",
      "Epoch [290/300], Loss: 0.002787819335949176\n",
      "Epoch [291/300], Loss: 0.0011258758282571611\n",
      "Epoch [292/300], Loss: 0.0015507834422629153\n",
      "Epoch [293/300], Loss: 0.0015933729007625418\n",
      "Epoch [294/300], Loss: 0.0012993110821844195\n",
      "Epoch [295/300], Loss: 0.0005942910404860976\n",
      "Epoch [296/300], Loss: 0.003983555393595982\n",
      "Epoch [297/300], Loss: 0.0020283636993063954\n",
      "Epoch [298/300], Loss: 0.0008827435805880867\n",
      "Epoch [299/300], Loss: 0.0013060513564309248\n",
      "Epoch [300/300], Loss: 0.0020811328153943387\n",
      "Training completed in: 3120.33 seconds\n",
      "Accuracy of the model on the 10000 test images: 83.54%\n"
     ]
    }
   ],
   "source": [
    "# 2 a \n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# CIFAR10 Dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "# ResNet Block\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet-10\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = ResNet(ResNetBlock, [1, 1, 1, 1]).to(device)  # ResNet-10\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader)}')\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f'Training completed in: {training_time:.2f} seconds')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the 10000 test images: {100 * correct / total}%')\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet10_cifar10.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/300], Loss: 1.2428373835047188\n",
      "Epoch [2/300], Loss: 0.7985277343970125\n",
      "Epoch [3/300], Loss: 0.6199493572077788\n",
      "Epoch [4/300], Loss: 0.5040087837278081\n",
      "Epoch [5/300], Loss: 0.4125342353644883\n",
      "Epoch [6/300], Loss: 0.33750051123749875\n",
      "Epoch [7/300], Loss: 0.2659018806960729\n",
      "Epoch [8/300], Loss: 0.20814111295258603\n",
      "Epoch [9/300], Loss: 0.16958500567080495\n",
      "Epoch [10/300], Loss: 0.13377332363916022\n",
      "Epoch [11/300], Loss: 0.11404700608104658\n",
      "Epoch [12/300], Loss: 0.10014884855748271\n",
      "Epoch [13/300], Loss: 0.08453160076690337\n",
      "Epoch [14/300], Loss: 0.07705410788802768\n",
      "Epoch [15/300], Loss: 0.0784136384738671\n",
      "Epoch [16/300], Loss: 0.06520066956770332\n",
      "Epoch [17/300], Loss: 0.0573893343762416\n",
      "Epoch [18/300], Loss: 0.05841569407322608\n",
      "Epoch [19/300], Loss: 0.05061913601985044\n",
      "Epoch [20/300], Loss: 0.051796716434108166\n",
      "Epoch [21/300], Loss: 0.04868681867124008\n",
      "Epoch [22/300], Loss: 0.04140785434450049\n",
      "Epoch [23/300], Loss: 0.04784006520774087\n",
      "Epoch [24/300], Loss: 0.04202932853009338\n",
      "Epoch [25/300], Loss: 0.04055494892418675\n",
      "Epoch [26/300], Loss: 0.03590800612182368\n",
      "Epoch [27/300], Loss: 0.0313971256378361\n",
      "Epoch [28/300], Loss: 0.03745365893239599\n",
      "Epoch [29/300], Loss: 0.03207284761296497\n",
      "Epoch [30/300], Loss: 0.034517075345718094\n",
      "Epoch [31/300], Loss: 0.030935159015707144\n",
      "Epoch [32/300], Loss: 0.028870836149736206\n",
      "Epoch [33/300], Loss: 0.027356046909431254\n",
      "Epoch [34/300], Loss: 0.028628396646901512\n",
      "Epoch [35/300], Loss: 0.0261327693182061\n",
      "Epoch [36/300], Loss: 0.025772602252295666\n",
      "Epoch [37/300], Loss: 0.02689528261413923\n",
      "Epoch [38/300], Loss: 0.022994542598084675\n",
      "Epoch [39/300], Loss: 0.02735520040187577\n",
      "Epoch [40/300], Loss: 0.023203007414691536\n",
      "Epoch [41/300], Loss: 0.02547482862629269\n",
      "Epoch [42/300], Loss: 0.0203989086825403\n",
      "Epoch [43/300], Loss: 0.018457427010619136\n",
      "Epoch [44/300], Loss: 0.025786103737058058\n",
      "Epoch [45/300], Loss: 0.019571929372792892\n",
      "Epoch [46/300], Loss: 0.018939525166595216\n",
      "Epoch [47/300], Loss: 0.021106932787104068\n",
      "Epoch [48/300], Loss: 0.019326832455546\n",
      "Epoch [49/300], Loss: 0.018138021583255927\n",
      "Epoch [50/300], Loss: 0.017718450939378794\n",
      "Epoch [51/300], Loss: 0.02237303611300865\n",
      "Epoch [52/300], Loss: 0.020715798661612984\n",
      "Epoch [53/300], Loss: 0.015361222267464099\n",
      "Epoch [54/300], Loss: 0.015958774241406227\n",
      "Epoch [55/300], Loss: 0.013015807393332482\n",
      "Epoch [56/300], Loss: 0.02012947056836778\n",
      "Epoch [57/300], Loss: 0.016901829859979563\n",
      "Epoch [58/300], Loss: 0.018271544814812964\n",
      "Epoch [59/300], Loss: 0.013978804482774096\n",
      "Epoch [60/300], Loss: 0.014535068796121172\n",
      "Epoch [61/300], Loss: 0.015192286201489298\n",
      "Epoch [62/300], Loss: 0.015932307525365518\n",
      "Epoch [63/300], Loss: 0.012555195492151582\n",
      "Epoch [64/300], Loss: 0.015921989529858047\n",
      "Epoch [65/300], Loss: 0.016925850698556986\n",
      "Epoch [66/300], Loss: 0.015237939715123392\n",
      "Epoch [67/300], Loss: 0.011906796133668516\n",
      "Epoch [68/300], Loss: 0.012840203281835955\n",
      "Epoch [69/300], Loss: 0.01480280339489426\n",
      "Epoch [70/300], Loss: 0.011809134883262925\n",
      "Epoch [71/300], Loss: 0.0140450613966179\n",
      "Epoch [72/300], Loss: 0.013790026789538294\n",
      "Epoch [73/300], Loss: 0.012372447706226045\n",
      "Epoch [74/300], Loss: 0.010969137197130841\n",
      "Epoch [75/300], Loss: 0.013356999328435214\n",
      "Epoch [76/300], Loss: 0.013785934173087242\n",
      "Epoch [77/300], Loss: 0.010260759964692117\n",
      "Epoch [78/300], Loss: 0.01404056827747583\n",
      "Epoch [79/300], Loss: 0.011471287240158319\n",
      "Epoch [80/300], Loss: 0.00943472050365068\n",
      "Epoch [81/300], Loss: 0.012275157247982917\n",
      "Epoch [82/300], Loss: 0.011535889007626724\n",
      "Epoch [83/300], Loss: 0.00917187158587871\n",
      "Epoch [84/300], Loss: 0.010411308278052533\n",
      "Epoch [85/300], Loss: 0.011551319652109827\n",
      "Epoch [86/300], Loss: 0.00990221198122652\n",
      "Epoch [87/300], Loss: 0.010853319406421511\n",
      "Epoch [88/300], Loss: 0.013437038065464138\n",
      "Epoch [89/300], Loss: 0.011191596671625304\n",
      "Epoch [90/300], Loss: 0.010711992930634977\n",
      "Epoch [91/300], Loss: 0.010804907898526162\n",
      "Epoch [92/300], Loss: 0.008997395138227575\n",
      "Epoch [93/300], Loss: 0.008326258168469564\n",
      "Epoch [94/300], Loss: 0.012011483056502075\n",
      "Epoch [95/300], Loss: 0.01027633318919419\n",
      "Epoch [96/300], Loss: 0.008551230015573752\n",
      "Epoch [97/300], Loss: 0.010424374272844334\n",
      "Epoch [98/300], Loss: 0.008284553059801535\n",
      "Epoch [99/300], Loss: 0.010282856907536688\n",
      "Epoch [100/300], Loss: 0.009266036981065916\n",
      "Epoch [101/300], Loss: 0.008615320649323738\n",
      "Epoch [102/300], Loss: 0.011614067067099346\n",
      "Epoch [103/300], Loss: 0.008418486347579655\n",
      "Epoch [104/300], Loss: 0.009422942821817307\n",
      "Epoch [105/300], Loss: 0.006832212285596662\n",
      "Epoch [106/300], Loss: 0.0083479664940755\n",
      "Epoch [107/300], Loss: 0.008686685576214907\n",
      "Epoch [108/300], Loss: 0.009675440462012916\n",
      "Epoch [109/300], Loss: 0.006925007387293069\n",
      "Epoch [110/300], Loss: 0.007304610699366025\n",
      "Epoch [111/300], Loss: 0.008740255077410832\n",
      "Epoch [112/300], Loss: 0.007934893022239752\n",
      "Epoch [113/300], Loss: 0.007450877180064029\n",
      "Epoch [114/300], Loss: 0.008285449986931646\n",
      "Epoch [115/300], Loss: 0.00652552633694766\n",
      "Epoch [116/300], Loss: 0.01007395186782553\n",
      "Epoch [117/300], Loss: 0.007629636045861881\n",
      "Epoch [118/300], Loss: 0.006284921066380798\n",
      "Epoch [119/300], Loss: 0.008419818768640498\n",
      "Epoch [120/300], Loss: 0.007386369203692442\n",
      "Epoch [121/300], Loss: 0.006812542242734982\n",
      "Epoch [122/300], Loss: 0.008273835911830507\n",
      "Epoch [123/300], Loss: 0.006974014465431956\n",
      "Epoch [124/300], Loss: 0.008132552444240479\n",
      "Epoch [125/300], Loss: 0.008082731391732174\n",
      "Epoch [126/300], Loss: 0.007306569763287066\n",
      "Epoch [127/300], Loss: 0.007893642312758268\n",
      "Epoch [128/300], Loss: 0.008119152125827941\n",
      "Epoch [129/300], Loss: 0.009895985731492911\n",
      "Epoch [130/300], Loss: 0.005067225317308744\n",
      "Epoch [131/300], Loss: 0.0063098971499091746\n",
      "Epoch [132/300], Loss: 0.006477034910602624\n",
      "Epoch [133/300], Loss: 0.007032684648925228\n",
      "Epoch [134/300], Loss: 0.009490629730932348\n",
      "Epoch [135/300], Loss: 0.0079947978777744\n",
      "Epoch [136/300], Loss: 0.005674632992383715\n",
      "Epoch [137/300], Loss: 0.007489396601364518\n",
      "Epoch [138/300], Loss: 0.0070072832002068056\n",
      "Epoch [139/300], Loss: 0.006345813968722086\n",
      "Epoch [140/300], Loss: 0.006658980542358368\n",
      "Epoch [141/300], Loss: 0.007335063245657593\n",
      "Epoch [142/300], Loss: 0.005802843543594051\n",
      "Epoch [143/300], Loss: 0.00424400223164528\n",
      "Epoch [144/300], Loss: 0.005809279409044616\n",
      "Epoch [145/300], Loss: 0.008088396342598645\n",
      "Epoch [146/300], Loss: 0.006581305647647064\n",
      "Epoch [147/300], Loss: 0.007634373772729388\n",
      "Epoch [148/300], Loss: 0.006143608907421976\n",
      "Epoch [149/300], Loss: 0.007085575277425513\n",
      "Epoch [150/300], Loss: 0.005607800319760727\n",
      "Epoch [151/300], Loss: 0.005508033535802858\n",
      "Epoch [152/300], Loss: 0.005640170564967422\n",
      "Epoch [153/300], Loss: 0.007989253138932834\n",
      "Epoch [154/300], Loss: 0.007246823892404084\n",
      "Epoch [155/300], Loss: 0.006220628291428586\n",
      "Epoch [156/300], Loss: 0.005925600921834891\n",
      "Epoch [157/300], Loss: 0.005976814688135637\n",
      "Epoch [158/300], Loss: 0.004890756235643894\n",
      "Epoch [159/300], Loss: 0.006415900884720547\n",
      "Epoch [160/300], Loss: 0.006523175556703686\n",
      "Epoch [161/300], Loss: 0.006909931885860008\n",
      "Epoch [162/300], Loss: 0.004040155024426604\n",
      "Epoch [163/300], Loss: 0.005951214651709935\n",
      "Epoch [164/300], Loss: 0.005327625419268222\n",
      "Epoch [165/300], Loss: 0.007043014897560525\n",
      "Epoch [166/300], Loss: 0.005506601147684208\n",
      "Epoch [167/300], Loss: 0.00487181938957342\n",
      "Epoch [168/300], Loss: 0.004620253831837575\n",
      "Epoch [169/300], Loss: 0.004485844322965137\n",
      "Epoch [170/300], Loss: 0.006334738611052359\n",
      "Epoch [171/300], Loss: 0.006017788135810276\n",
      "Epoch [172/300], Loss: 0.004401599486189767\n",
      "Epoch [173/300], Loss: 0.00527927726712271\n",
      "Epoch [174/300], Loss: 0.0060946902982983685\n",
      "Epoch [175/300], Loss: 0.005496379027864778\n",
      "Epoch [176/300], Loss: 0.006008694651678139\n",
      "Epoch [177/300], Loss: 0.0054330283677397236\n",
      "Epoch [178/300], Loss: 0.006369812215212696\n",
      "Epoch [179/300], Loss: 0.004606585070811026\n",
      "Epoch [180/300], Loss: 0.005049783915969865\n",
      "Epoch [181/300], Loss: 0.004890659732265525\n",
      "Epoch [182/300], Loss: 0.003987048354916563\n",
      "Epoch [183/300], Loss: 0.0038320122462382838\n",
      "Epoch [184/300], Loss: 0.006129093667371777\n",
      "Epoch [185/300], Loss: 0.005632249168014884\n",
      "Epoch [186/300], Loss: 0.004719516986343395\n",
      "Epoch [187/300], Loss: 0.005258336586748433\n",
      "Epoch [188/300], Loss: 0.004276976835583233\n",
      "Epoch [189/300], Loss: 0.003658832339247672\n",
      "Epoch [190/300], Loss: 0.005471449688116954\n",
      "Epoch [191/300], Loss: 0.00493635968611167\n",
      "Epoch [192/300], Loss: 0.005832362377536916\n",
      "Epoch [193/300], Loss: 0.0066908406718762204\n",
      "Epoch [194/300], Loss: 0.0039318273622213715\n",
      "Epoch [195/300], Loss: 0.004531476087051602\n",
      "Epoch [196/300], Loss: 0.004144744978557786\n",
      "Epoch [197/300], Loss: 0.0052049275188557976\n",
      "Epoch [198/300], Loss: 0.005179179286427013\n",
      "Epoch [199/300], Loss: 0.004752920043230911\n",
      "Epoch [200/300], Loss: 0.004113215790092864\n",
      "Epoch [201/300], Loss: 0.0038519565146827536\n",
      "Epoch [202/300], Loss: 0.005378846784430055\n",
      "Epoch [203/300], Loss: 0.0056199479746642704\n",
      "Epoch [204/300], Loss: 0.004391442492670699\n",
      "Epoch [205/300], Loss: 0.004756016172622893\n",
      "Epoch [206/300], Loss: 0.004955146890477262\n",
      "Epoch [207/300], Loss: 0.0053303428470490065\n",
      "Epoch [208/300], Loss: 0.004292100909829362\n",
      "Epoch [209/300], Loss: 0.003559062537414834\n",
      "Epoch [210/300], Loss: 0.004340875273363366\n",
      "Epoch [211/300], Loss: 0.005186992427801404\n",
      "Epoch [212/300], Loss: 0.004896887173446497\n",
      "Epoch [213/300], Loss: 0.0028656345113362443\n",
      "Epoch [214/300], Loss: 0.005697450095288127\n",
      "Epoch [215/300], Loss: 0.007146120092472564\n",
      "Epoch [216/300], Loss: 0.003861745366894398\n",
      "Epoch [217/300], Loss: 0.0031219214354272435\n",
      "Epoch [218/300], Loss: 0.003549640049521519\n",
      "Epoch [219/300], Loss: 0.005562992354953748\n",
      "Epoch [220/300], Loss: 0.00455784354884218\n",
      "Epoch [221/300], Loss: 0.004975068163321223\n",
      "Epoch [222/300], Loss: 0.0034506701314126775\n",
      "Epoch [223/300], Loss: 0.005157152304080622\n",
      "Epoch [224/300], Loss: 0.004691938486297591\n",
      "Epoch [225/300], Loss: 0.0036384193145016447\n",
      "Epoch [226/300], Loss: 0.003433496441605699\n",
      "Epoch [227/300], Loss: 0.0034405428736269113\n",
      "Epoch [228/300], Loss: 0.005571253689853736\n",
      "Epoch [229/300], Loss: 0.004370504874879103\n",
      "Epoch [230/300], Loss: 0.0040652698466327975\n",
      "Epoch [231/300], Loss: 0.004433858037548866\n",
      "Epoch [232/300], Loss: 0.0042076190768772104\n",
      "Epoch [233/300], Loss: 0.004482265798482144\n",
      "Epoch [234/300], Loss: 0.004238571978221251\n",
      "Epoch [235/300], Loss: 0.0036334049943098977\n",
      "Epoch [236/300], Loss: 0.0032623467274992676\n",
      "Epoch [237/300], Loss: 0.004027213512763131\n",
      "Epoch [238/300], Loss: 0.004372947606070131\n",
      "Epoch [239/300], Loss: 0.004533830167292428\n",
      "Epoch [240/300], Loss: 0.003955574672682138\n",
      "Epoch [241/300], Loss: 0.0048952042968278394\n",
      "Epoch [242/300], Loss: 0.0035231845952233567\n",
      "Epoch [243/300], Loss: 0.004070414663820565\n",
      "Epoch [244/300], Loss: 0.0026153896932877814\n",
      "Epoch [245/300], Loss: 0.004161491545147628\n",
      "Epoch [246/300], Loss: 0.004046917903229982\n",
      "Epoch [247/300], Loss: 0.0038971415138575397\n",
      "Epoch [248/300], Loss: 0.0037280894196992684\n",
      "Epoch [249/300], Loss: 0.003411371018355852\n",
      "Epoch [250/300], Loss: 0.004031544033831446\n",
      "Epoch [251/300], Loss: 0.004384909179832495\n",
      "Epoch [252/300], Loss: 0.003343892229729288\n",
      "Epoch [253/300], Loss: 0.002640445858607482\n",
      "Epoch [254/300], Loss: 0.0047468779026727526\n",
      "Epoch [255/300], Loss: 0.003921300934715819\n",
      "Epoch [256/300], Loss: 0.0028506808237446658\n",
      "Epoch [257/300], Loss: 0.0039791005397141225\n",
      "Epoch [258/300], Loss: 0.0051957256913613215\n",
      "Epoch [259/300], Loss: 0.003411935853803022\n",
      "Epoch [260/300], Loss: 0.0025100426319907237\n",
      "Epoch [261/300], Loss: 0.003944508428274059\n",
      "Epoch [262/300], Loss: 0.0030180672428959453\n",
      "Epoch [263/300], Loss: 0.003935408076120595\n",
      "Epoch [264/300], Loss: 0.003454041097832771\n",
      "Epoch [265/300], Loss: 0.005209171959603844\n",
      "Epoch [266/300], Loss: 0.004517641879785266\n",
      "Epoch [267/300], Loss: 0.0032904912058500397\n",
      "Epoch [268/300], Loss: 0.0034010315736815134\n",
      "Epoch [269/300], Loss: 0.003133074864114937\n",
      "Epoch [270/300], Loss: 0.0039732099925657485\n",
      "Epoch [271/300], Loss: 0.003508251811125375\n",
      "Epoch [272/300], Loss: 0.003015285637434637\n",
      "Epoch [273/300], Loss: 0.0033656170178207817\n",
      "Epoch [274/300], Loss: 0.0035766114238970233\n",
      "Epoch [275/300], Loss: 0.003444649472584742\n",
      "Epoch [276/300], Loss: 0.003528922985144359\n",
      "Epoch [277/300], Loss: 0.003833466581801943\n",
      "Epoch [278/300], Loss: 0.003889015248892024\n",
      "Epoch [279/300], Loss: 0.003527710263650246\n",
      "Epoch [280/300], Loss: 0.0035359892827060454\n",
      "Epoch [281/300], Loss: 0.0038339856462506327\n",
      "Epoch [282/300], Loss: 0.002950224634091369\n",
      "Epoch [283/300], Loss: 0.0022828544489142794\n",
      "Epoch [284/300], Loss: 0.00391963615523186\n",
      "Epoch [285/300], Loss: 0.002869765666483618\n",
      "Epoch [286/300], Loss: 0.003077346229969587\n",
      "Epoch [287/300], Loss: 0.005171831694022277\n",
      "Epoch [288/300], Loss: 0.0026298814080032563\n",
      "Epoch [289/300], Loss: 0.0040417884690270535\n",
      "Epoch [290/300], Loss: 0.0034962602762966537\n",
      "Epoch [291/300], Loss: 0.002739635080399156\n",
      "Epoch [292/300], Loss: 0.0025784661004905657\n",
      "Epoch [293/300], Loss: 0.0045550049493478855\n",
      "Epoch [294/300], Loss: 0.003018389242572132\n",
      "Epoch [295/300], Loss: 0.0025897542915848198\n",
      "Epoch [296/300], Loss: 0.003785493771769455\n",
      "Epoch [297/300], Loss: 0.0030977361725969873\n",
      "Epoch [298/300], Loss: 0.0038838750616179323\n",
      "Epoch [299/300], Loss: 0.0026618347569461534\n",
      "Epoch [300/300], Loss: 0.004040194241457759\n",
      "Training completed in: 3270.87 seconds\n",
      "Accuracy on test images: 86.29%\n"
     ]
    }
   ],
   "source": [
    "#2b\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001  # For weight decay\n",
    "dropout_rate = 0.3    # For dropout\n",
    "\n",
    "# CIFAR10 Dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "# ResNet Block\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_dropout=False):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)  # Batch Normalization\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)  # Batch Normalization\n",
    "        self.dropout = nn.Dropout(dropout_rate) if use_dropout else nn.Identity()\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)  # Batch Normalization\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.dropout(out)  # Apply dropout\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet-10 with regularization options\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, use_dropout=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)  # Batch Normalization\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, use_dropout=use_dropout)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, use_dropout=use_dropout)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, use_dropout=use_dropout)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, use_dropout=use_dropout)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride, use_dropout):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride, use_dropout=use_dropout))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# Select the regularization method here\n",
    "use_weight_decay = False\n",
    "use_dropout = True  # Set to False for experiments without dropout\n",
    "use_batch_norm = True  # Batch Normalization is integrated in the model\n",
    "\n",
    "model = ResNet(ResNetBlock, [1, 1, 1, 1], use_dropout=use_dropout).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "# Adjust weight decay in the optimizer for L2 regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay if use_weight_decay else 0)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader)}')\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f'Training completed in: {training_time:.2f} seconds')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on test images: {accuracy}%')\n",
    "\n",
    "# Save the model with a different name for each regularization method\n",
    "model_path = 'C:\\\\Users\\\\hanba\\\\OneDrive\\\\Documents\\\\unc c\\\\introML\\\\MLHWCore\\\\HW7\\\\resnet10_cifar10_regularized.pth'\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
