# -*- coding: utf-8 -*-
"""HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16vxeFSjYCaYc1nsVvuHXzU7eEz3ne8jp
"""

import numpy as np
import pandas as pd

# Data Visualisation

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

#mounting google drive
from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/IntroML/content/Housing.csv'
hs = pd.DataFrame(pd.read_csv(file_path))
display(hs)

# List of variables to map

varlist =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']

# Defining the map function
def binary_map(x):
    return x.map({'yes': 1, 'no': 0})

# Applying the function to the housing list
hs[varlist] = hs[varlist].apply(binary_map)
hs.head()

np.random.seed(0)
ts, vs = train_test_split(hs, train_size = 0.8, test_size = 0.2, random_state = 100)

y_ts = ts.pop('price')
y_vs = vs.pop('price')

"""Problem 1.a & 1.b"""

# Function Definitions
def hypothesis(X, theta):
    return np.dot(X, theta)

def compute_cost(X, y, theta):
    m = len(y)
    predictions = hypothesis(X, theta)
    sq_errors = (predictions - y) ** 2
    return 1 / (2 * m) * np.sum(sq_errors)

def gradient_descent(X_train, y_ts, X_test, y_vs, theta, alpha, num_iters):
    m_train = len(X_train)
    m_test = len(X_test)

    J_history = []
    J_test_history = []

    for i in range(num_iters):
        gradients = ( 1 / m_train) * np.dot(X_train.T, hypothesis(X_train, theta) - y_ts)
        theta = theta - alpha * gradients

        J_history.append(compute_cost(X_train, y_ts, theta))
        J_test_history.append(compute_cost(X_test, y_vs, theta))

    return theta, J_history, J_test_history

# Problem 1.a
input = ['area',	'bedrooms',	'bathrooms',	'stories', 'parking']
x_train = ts[input]
x_test = vs[input]

X_ts_a = np.c_[np.ones(len(x_train)), x_train]
X_vs_a = np.c_[np.ones(len(x_test)), x_test]

alpha_a = 0.01 # You might need to adjust this value since features are not normalized
num_iters = 35
theta = np.zeros((X_ts_a.shape[1]))
theta_a, J_history_a, J_test_history_a = gradient_descent(X_ts_a, y_ts, X_vs_a, y_vs, theta, alpha_a, num_iters)

# Problem 1.b
input = ts.columns.tolist()[:-1]
x_train = ts[input]
x_test = vs[input]

X_ts_b = np.column_stack((np.ones(x_train.shape[0]), x_train))
X_vs_b = np.column_stack((np.ones(x_test.shape[0]), x_test))

alpha_b = 0.01
theta_b = np.zeros((X_ts_b.shape[1]))
theta_b, J_history_b, J_test_history_b = gradient_descent(X_ts_b, y_ts, X_vs_b, y_vs, theta_b, alpha_b, num_iters)

# Printing Theta Values for 1.a & 1.b
print("Theta values for Problem 1.a are:")
print(theta_a)

print("\nTheta values for Problem 1.b are:")
print(theta_b)

# Plotting for 1.a
plt.figure()
plt.plot(J_history_a, label='Training Loss 1.a')
plt.plot(J_test_history_a, label='Test Loss 1.a', linestyle = '--')
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.legend()
plt.title("Loss curves for Problem 1.a")
plt.show()

# Plotting for 1.b
plt.figure()
plt.plot(J_history_b, label='Training Loss 1.b')
plt.plot(J_test_history_b, label='Test Loss 1.b', linestyle='--')
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.legend()
plt.title("Loss curves for Problem 1.b")
plt.show()

"""Problem 2.a & 2.b"""

import numpy as np
import matplotlib.pyplot as plt

# Function Definitions
def hypothesis(X, theta):
    return np.dot(X, theta)

def compute_cost(X, y, theta):
    m = len(y)
    predictions = hypothesis(X, theta)
    sq_errors = (predictions - y) ** 2
    return 1 / (2 * m) * np.sum(sq_errors)

def gradient_descent(X_train, y_ts, X_test, y_vs, theta, alpha, num_iters):
    m_train = len(X_train)
    m_test = len(X_test)

    J_history = []
    J_test_history = []

    for i in range(num_iters):
        gradients = ( 1 / m_train) * np.dot(X_train.T, hypothesis(X_train, theta) - y_ts)
        theta = theta - alpha * gradients

        J_history.append(compute_cost(X_train, y_ts, theta))
        J_test_history.append(compute_cost(X_test, y_vs, theta))

    return theta, J_history, J_test_history

def normalize(data):
    return (data - data.min()) / (data.max() - data.min()), data.min(), data.max()

def standardize(data):
    return (data - data.mean()) / data.std(), data.mean(), data.std()

# Problem 2.a
input = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']

# Using Normalization
x_train = ts[input]
x_test = vs[input]

x_train_norm, min_vals, max_vals = normalize(x_train)
x_test_norm = (x_test - min_vals) / (max_vals - min_vals)

X_ts_a_norm = np.c_[np.ones(len(x_train_norm)), x_train_norm]
X_vs_a_norm = np.c_[np.ones(len(x_test_norm)), x_test_norm]

alpha_a = 0.01
num_iters = 200
theta = np.zeros(X_ts_a_norm.shape[1])
theta_a_norm, J_history_a_norm, J_test_history_a_norm = gradient_descent(X_ts_a_norm, y_ts, X_vs_a_norm, y_vs, theta, alpha_a, num_iters)

# Using Standardization
x_train_std, mean, std_dev = standardize(x_train)
x_test_std = (x_test - mean) / std_dev

X_ts_a_std = np.c_[np.ones(len(x_train_std)), x_train_std]
X_vs_a_std = np.c_[np.ones(len(x_test_std)), x_test_std]

theta = np.zeros(X_ts_a_std.shape[1])
theta_a_std, J_history_a_std, J_test_history_a_std = gradient_descent(X_ts_a_std, y_ts, X_vs_a_std, y_vs, theta, alpha_a, num_iters)

# Problem 2.b
input = ts.columns.tolist()[:-1]

# Using Normalization
x_train = ts[input]
x_test = vs[input]

x_train_norm, min_vals, max_vals = normalize(x_train)
x_test_norm = (x_test - min_vals) / (max_vals - min_vals)

X_ts_b_norm = np.c_[np.ones(len(x_train_norm)), x_train_norm]
X_vs_b_norm = np.c_[np.ones(len(x_test_norm)), x_test_norm]

alpha_b = 0.01
theta_b = np.zeros(X_ts_b_norm.shape[1])
theta_b_norm, J_history_b_norm, J_test_history_b_norm = gradient_descent(X_ts_b_norm, y_ts, X_vs_b_norm, y_vs, theta_b, alpha_b, num_iters)

# Using Standardization
x_train_std, mean, std_dev = standardize(x_train)
x_test_std = (x_test - mean) / std_dev

X_ts_b_std = np.c_[np.ones(len(x_train_std)), x_train_std]
X_vs_b_std = np.c_[np.ones(len(x_test_std)), x_test_std]

theta_b = np.zeros(X_ts_b_std.shape[1])
theta_b_std, J_history_b_std, J_test_history_b_std = gradient_descent(X_ts_b_std, y_ts, X_vs_b_std, y_vs, theta_b, alpha_b, num_iters)

# Plotting for 2.a
plt.figure(figsize=(12, 6))
plt.plot(J_history_a_norm, label='Training Loss 2.a (Normalized)')
plt.plot(J_test_history_a_norm, label='Test Loss 2.a (Normalized)', linestyle='--')
plt.plot(J_history_a_std, label='Training Loss 2.a (Standardized)', linestyle='-.')
plt.plot(J_test_history_a_std, label='Test Loss 2.a (Standardized)', linestyle=':')
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.legend()
plt.title("Loss curves for Problem 2.a")
plt.show()

# Plotting for 2.b
plt.figure(figsize=(12, 6))
plt.plot(J_history_b_norm, label='Training Loss 2.b (Normalized)')
plt.plot(J_test_history_b_norm, label='Test Loss 2.b (Normalized)', linestyle='--')
plt.plot(J_history_b_std, label='Training Loss 2.b (Standardized)', linestyle='-.')
plt.plot(J_test_history_b_std, label='Test Loss 2.b (Standardized)', linestyle=':')
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.legend()
plt.title("Loss curves for Problem 2.b")
plt.show()

# Printing Theta Values for 2.a & 2.b
print("Theta values for Problem 2.a using Normalization are:")
print(theta_a_norm)
print("\nTheta values for Problem 2.a using Standardization are:")
print(theta_a_std)

print("\nTheta values for Problem 2.b using Normalization are:")
print(theta_b_norm)
print("\nTheta values for Problem 2.b using Standardization are:")
print(theta_b_std)

"""Problem 3a & 3b"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Assuming 'hs' is your dataframe containing the data
variable_list = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']

np.random.seed(1)
df_training, df_validation = train_test_split(hs, train_size=0.8, test_size=0.2, random_state=100)

# Normalization
normalize = MinMaxScaler()
norm_training = normalize.fit_transform(df_training)
norm_validation = normalize.transform(df_validation)

norm_training = pd.DataFrame(norm_training, columns=df_training.columns)
norm_validation = pd.DataFrame(norm_validation, columns=df_validation.columns)

# Standardization
standardize = StandardScaler()
std_training = standardize.fit_transform(df_training)
std_validation = standardize.transform(df_validation)

std_training = pd.DataFrame(std_training, columns=df_training.columns)
std_validation = pd.DataFrame(std_validation, columns=df_validation.columns)

# Non-feature scaled Outputs
y_train = df_training.pop('price')
y_valid = df_validation.pop('price')

# Normalized Outputs
norm_y_train = norm_training.pop('price')
norm_y_valid = norm_validation.pop('price')

# Standardized Outputs
std_y_train = std_training.pop('price')
std_y_valid = std_validation.pop('price')

def train_model(inputs, x_training, x_validation, y_train, y_valid, learning_rate, iterations, lambda_=0.1):
    training_inputs = x_training[inputs]
    x_train = np.c_[np.ones((len(training_inputs), 1)), training_inputs]

    validation_inputs = x_validation[inputs]
    x_valid = np.c_[np.ones((len(validation_inputs), 1)), validation_inputs]

    n = x_train.shape[1]
    m_train = len(x_train)
    m_valid = len(x_valid)
    theta = np.zeros(n)

    train_losses = []
    valid_losses = []

    for i in range(iterations):
        h_theta = x_train.dot(theta)
        error = np.subtract(h_theta, y_train)

        gradient_reg = (lambda_ / m_train) * theta
        gradient_reg[0] = 0

        gradient = (1 / m_train) * (x_train.transpose().dot(error)) + gradient_reg
        theta -= (learning_rate * gradient)

        train_loss = 1 / (2 * m_train) * (np.sum(np.square((h_theta - y_train))) + lambda_ / 2 * np.sum(np.square(theta[1:])))
        valid_loss = 1 / (2 * m_valid) * np.sum(np.square((x_valid.dot(theta) - y_valid)))

        train_losses.append(train_loss)
        valid_losses.append(valid_loss)

    return train_losses, valid_losses

inputs = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']
learning_rate = [0.01]
iterations = 200
lambda_ = 0.1  # You can experiment with this regularization strength

def plot_losses(train_losses, valid_losses, scaling_type, learning_rate):
    plt.figure(figsize=(10, 6))

    plt.plot(train_losses, label=f"{scaling_type} Training Loss", linestyle='-', marker='o')
    plt.plot(valid_losses, label=f"{scaling_type} Validation Loss", linestyle='-', marker='x')

    plt.xlabel("Iteration")
    plt.ylabel("Loss")
    plt.title(f"Loss using {scaling_type} Inputs and Learning Rate of {learning_rate}")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Train and plot for normalized inputs with learning_rate = 0.1
norm_train_losses, norm_valid_losses = train_model(inputs, norm_training, norm_validation, norm_y_train, norm_y_valid, 0.1, iterations, lambda_)
plot_losses(norm_train_losses, norm_valid_losses, "Normalized", 0.1)

# Train and plot for standardized inputs with learning_rate = 0.1
std_train_losses, std_valid_losses = train_model(inputs, std_training, std_validation, std_y_train, std_y_valid, 0.1, iterations, lambda_)
plot_losses(std_train_losses, std_valid_losses, "Standardized", 0.1)